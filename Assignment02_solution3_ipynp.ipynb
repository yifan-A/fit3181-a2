{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/coco12323/first-git/blob/main/Assignment02_solution_ipynp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq-ivp_W9jjz"
   },
   "source": [
    "# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (T3, 2020)</span>\n",
    "***\n",
    "*Chief Examiner:* Prof **Dinh Phung** | dinh.phung@monash.edu <br/>\n",
    "*Lecturer:* Dr **Peibo Duan** | peibo.duan@monash.edu <br/>\n",
    "*Tutor:* Mr **Qing Ye** | qing.ye@monash.edu <br/>\n",
    "\n",
    "<br/>\n",
    "Department of Data Science and AI, Faculty of Information Technology, Monash University, Australia <br/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9WGoOYr9jj2"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Assignment 02: Neural Embedding and Sequence Modelling</span>\n",
    "### Due: <span style=\"color:red\">11:59pm 29 August 2020</span>  (Sunday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **individual** assignment. It contributes **20%** to you final mark. Read the assignment instruction carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01arsSlN9jj2"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Instructions</span>\n",
    "\n",
    "This notebook has been prepared for your to complete Assignment 2. The theme of this assignment is about practical machine learning knowledge and skills in deep neural networks, word embedding and text analytics. Some sections have been partially completed to help you get\n",
    "started. **The total marks for this notebook is 110**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QVZdZ239jj2"
   },
   "source": [
    "* Before you start, read the entire notebook carefully once to understand what you need to do. <br><br>\n",
    "* For each cell marked with **#YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL**, there will be places where you **must** supply your own codes when instructed. <br>\n",
    "\n",
    "This assignment contains **four** parts:\n",
    "\n",
    "* Part 1: Questions on downloading and preprocessing data **[10 points]**\n",
    "* Part 2: Questions on using Word2Vect to transform texts to vectors **[20 points]**\n",
    "* Part 3: Coding assessment on Text CNN for sequence modeling and neural embedding **[15 points]**\n",
    "* Part 4: Coding assessment on RNNs for sequence modeling and neural embedding **[65 points]**\n",
    "\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the lectures and tutorials sessions covered from Week 5 to 6. You are strongly recommended to go through these contents thoroughly which might help you to complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoBw3WYt9jj3"
   },
   "source": [
    "## <span style=\"color:#0b486b\">What to submit</span>\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. **By the due date, you are required to submit one  <span style=\"color:red; font-weight:bold\">single zip file, named xxx_assignment02_solution.zip</span> where `xxx` is your student ID, to the corresponding Assignment (Dropbox) in Moodle**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQZDk5aZ9jj3"
   },
   "source": [
    "***For example, if your student ID is <span style=\"color:red; font-weight:bold\">12356</span>, then gather all of your assignment solution to folder, create a zip file named <span style=\"color:red; font-weight:bold\">123456_assignment02_solution.zip</span> and submit this file.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4fEMi5K9jj4"
   },
   "source": [
    "Within this zip folder, you **must** submit the following files:\n",
    "1.\t**Assignment02_solution.ipynp**:  this is your Python notebook solution source file.\n",
    "1.\t**Assignment02_output.html**: this is the output of your Python notebook solution *exported* in html format.\n",
    "1.\tAny **extra files or folder** needed to complete your assignment (e.g., images used in your answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEVB6Km29jj4"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Download and preprocess the data</span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdFM73fQ9jj4"
   },
   "source": [
    "The dataset we use for this assignment is a small [Question Classification](http://cogcomp.org/Data/QA/QC/) dataset which was used in the research paper on the topic of ***Question Classification*** (QC) published by Roth and Xin in 2002 (which has received close to 1000 citations till today). Note that a pdf copy of this paper has been downloaded into this assignment folder (**`Roth_Xin_coling02.pdf`**).\n",
    "\n",
    "The train set consists of $1,000$ questions belonging to 6 coarse question categories, including:\n",
    "- abbreviation (ABBR), \n",
    "- entity (ENTY), \n",
    "- description (DESC), \n",
    "- human (HUM), \n",
    "- location (LOC) and \n",
    "- numeric (NUM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJi2QQyo9jj5"
   },
   "source": [
    "Preprocessing data is an inital and important step in any machine learning or deep learning projects. The following *DataManager* class helps you to download data and preprocess data for the later steps of a deep learning project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXpt55Cn9jj6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, verbose=True, maxlen= 50, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = maxlen\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "        \n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose= True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "    \n",
    "    def read_data(self, dir_name, file_names):\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            self.str_questions= list(); self.str_labels= list()\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str= row.split(\":\")\n",
    "                    label, question= row_str[0], row_str[1]\n",
    "                    question= question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len= len(self.str_questions[-1])\n",
    "         \n",
    "        # turns labels into numbers\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes= le.classes_\n",
    "        self.num_classes= len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(\"Labels {}\\n\\n\".format(self.str_classes))\n",
    "    \n",
    "    def manipulate_data(self):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.str_questions)\n",
    "        self.numeral_data = tokenizer.texts_to_sequences(self.str_questions)\n",
    "        self.numeral_data = tf.keras.preprocessing.sequence.pad_sequences(self.numeral_data, padding='post', truncating= 'post', maxlen= self.maxlen)##填充\n",
    "        self.word2idx = tokenizer.word_index\n",
    "        self.word2idx = {k:v for k,v in self.word2idx.items()}\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "    \n",
    "    def train_valid_split(self, train_ratio=0.9):\n",
    "        idxs = np.random.permutation(np.arange(len(self.str_questions)))\n",
    "        train_size = int(train_ratio*len(idxs)) +1\n",
    "        self.train_str_questions, self.valid_str_questions = self.str_questions[0:train_size], self.str_questions[train_size:]#str句子\n",
    "        self.train_numeral_data, self.valid_numeral_data = self.numeral_data[0:train_size], self.numeral_data[train_size:]#序列化的句子\n",
    "        self.train_numeral_labels, self.valid_numeral_labels = self.numeral_labels[0:train_size], self.numeral_labels[train_size:]#label\n",
    "        self.tf_train_set = tf.data.Dataset.from_tensor_slices((self.train_numeral_data, self.train_numeral_labels))\n",
    "        self.tf_valid_set = tf.data.Dataset.from_tensor_slices((self.valid_numeral_data, self.valid_numeral_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCC7eiF09jj9",
    "outputId": "189ab57b-f752-4f06-b232-33f38a19ad51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloaded successfully train_1000.label\n",
      "Downloaded successfully TREC_10.label\n",
      "\n",
      "Sample questions... \n",
      "\n",
      "['dist how far is it from denver to aspen ?', 'city what county is modesto , california in ?', 'desc who was galileo ?', 'def what is an atom ?', 'date when did hawaii become a state ?']\n",
      "Labels ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "DataManager.maybe_download(\"Data\", \"train_1000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "DataManager.maybe_download(\"Data\", \"TREC_10.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "\n",
    "dm = DataManager(maxlen=100)\n",
    "dm.read_data(\"Data/\", [\"train_1000.label\", \"TREC_10.label\"])   # read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF481IyF9jj-"
   },
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_split(train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeHGLeoK9jj_"
   },
   "source": [
    "You now have a data manager, named *dm* containing the training and validiation sets in both text and numeric forms. Your task is to play around and read this code to figure out the meanings of some important attributes that will be used in the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QAlnQ8x9jj_"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 1.1**</span> \n",
    "**What is the purpose of `self.train_str_questions` and `self.train_numeral_labels`? Write your code to print out the first five questions with labels in the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8jYOpDF9jkA"
   },
   "source": [
    "#Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmhs0gB5_Wmt"
   },
   "source": [
    "self.train_str_questions are text questions in string format\n",
    "\n",
    "self.train_numeral_labels are labels in numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZELF8PcT_G0z",
    "outputId": "f9404c93-eff4-4bba-8ace-e61eba5ff7b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'dist how far is it from denver to aspen ?', 5),\n",
       " (b'city what county is modesto , california in ?', 4),\n",
       " (b'desc who was galileo ?', 3),\n",
       " (b'def what is an atom ?', 1),\n",
       " (b'date when did hawaii become a state ?', 5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first five questions with labels in the training set.\n",
    "my_train_set = tf.data.Dataset.from_tensor_slices((dm.train_str_questions, dm.train_numeral_labels))\n",
    "list(my_train_set.as_numpy_iterator())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxozP0E99jkA"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 1.2**</span> \n",
    "**What is the purpose of `self.train_numeral_data`? Write your code to print out the first five questions in the numeric format with labels in the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqCkwTau9jkA"
   },
   "source": [
    "#Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKoPSy8XAke5"
   },
   "source": [
    "self.train_numeral_data is the text question in numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FU4w4dK39jkB",
    "outputId": "ae30aed7-9699-446e-f076-7f30b32ba9e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 27,  14,  73,   3, 115,  39, 305,  20, 306,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 5),\n",
       " (array([ 18,   1, 153,   3, 307, 308,   6,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 4),\n",
       " (array([ 42,  12,   8, 309,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 3),\n",
       " (array([  4,   1,   3,  40, 310,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 1),\n",
       " (array([ 11,  17,  16, 154,  90,  10,  21,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 5)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "list(dm.tf_train_set.as_numpy_iterator())[:5]#array[numeral,labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opVK-UuI9jkB"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 1.3**</span> \n",
    "**What is the purpose of two dictionaries: `self.word2idx` and `self.idx2word`? Write your code to print out the first five key-value pairs of those dictionaries.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSY2xUbz9jkB"
   },
   "source": [
    "#Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XtknyiiA9Fb"
   },
   "source": [
    "self.word2idx is the sequence number corresponding to the word in this question\n",
    "\n",
    "self.idx2word is the word in the question corresponding to the serial number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3MsMjJbCEo5",
    "outputId": "a4567257-a4b4-426a-f00d-7b44289cf344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'what'}\n",
      "{2: 'the'}\n",
      "{3: 'is'}\n",
      "{4: 'def'}\n",
      "{5: 'of'}\n"
     ]
    }
   ],
   "source": [
    "#the first five key-value pairs of those dictionaries\n",
    "for i,(k,v) in enumerate(dm.idx2word.items()):\n",
    "    if i in range(0,5):\n",
    "        print({k:v},end=\"\"+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spmqJMQI9jkB"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 1.4**</span> \n",
    "**What is the purpose of `self.tf_train_set`? Write your code to print out the first five items of `self.tf_train_set`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vDLPBl-9jkB"
   },
   "source": [
    "#Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAWbeHTkC9WV"
   },
   "source": [
    "self.tf_train_set is the populated train_numeral_data and self.train_numeral_labels\n",
    "\n",
    "Digitized text questions and labels for the saved training set\n",
    "\n",
    "Save in the format of array[train_numeral_data, self.train_numeral_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqK4cNCF9jkC",
    "outputId": "40df241c-f6fa-4fa5-ed3d-6d2f63b8f345"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 27,  14,  73,   3, 115,  39, 305,  20, 306,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 5),\n",
       " (array([ 18,   1, 153,   3, 307, 308,   6,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 4),\n",
       " (array([ 42,  12,   8, 309,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 3),\n",
       " (array([  4,   1,   3,  40, 310,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 1),\n",
       " (array([ 11,  17,  16, 154,  90,  10,  21,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first five items of self.tf_train_set\n",
    "list(dm.tf_train_set.as_numpy_iterator())[:5]#array[numeral,labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4dRZjJx9jkC"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 1.5**</span> \n",
    "**What is the purpose of `self.tf_valid_set`? Write your code to print out the first five items of `self.tf_valid_set`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnApZQWs9jkC"
   },
   "source": [
    "#Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azQ1P958DhCY"
   },
   "source": [
    "self.tf_valid_set is the populated valid_numeral_data and self.valid_numeral_labels\n",
    "\n",
    "Digitized text questions and labels for the saved validation set\n",
    "\n",
    "Save in the format of array[valid_numeral_data, self.valid_numeral_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJPEcAs-9jkC",
    "outputId": "b74ab481-19f5-4224-c061-3a482141fc0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 32,   1,   3,   2, 921, 922, 923, 924,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 2),\n",
       " (array([ 11,  17,   3, 925,  15,  69,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 5),\n",
       " (array([ 52,   1,  22,   2, 926, 301,  89,  24,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 0),\n",
       " (array([ 27,  14, 128,   3,   2, 927,  47,   6, 251,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 5),\n",
       " (array([ 18,   1,  18,  15, 258,   3,  36,   2, 293,  37,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32), 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first five items of self.tf_valid_set.\n",
    "list(dm.tf_valid_set.as_numpy_iterator())[:5]#array[numeral,labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIKYQDZZ9jkC"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Using Word2Vect to transform texts to vectors </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 20 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DD1YzWk9jkC"
   },
   "source": [
    "In this part, you will be assessed on how to use a pretrained Word2Vect model for realizing a machine learning task. Basically, you will use this pretrained Word2Vect to transform the questions in the above dataset stored in the *data manager object dm* to numeric form for training a Support Vector Machine in sckit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqeiXigi9jkC"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PStJFWhP9jkD"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.1**</span> \n",
    "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsmcYzug9jkD",
    "outputId": "6537edf9-0583-4476-a417-b3a34ad224a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "info = api.info()  # show info about available models/datasets\n",
    "word2vect = api.load(\"glove-wiki-gigaword-100\")#Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6P4lybqS9jkD"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.2**</span> \n",
    "\n",
    "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[3 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4v6Fz6g9jkD"
   },
   "outputs": [],
   "source": [
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        vector = word2vect[word]#Insert your code here\n",
    "    except: #word not in the vocabulary\n",
    "        vector = np.zeros((100,), dtype=\"float32\")#Insert your code here\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upCYgSsZ9jkD"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.3**</span> \n",
    "\n",
    "**Write the code for the function *get_sentence_vector(sentence, important_score=None, model= None)*. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
    "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[L]\\times v[L]$ where $L$ is the length of the sentence and $v[i]$ is the vector of the word $i-th$ in this sentence.\n",
    "\n",
    "**Note that if *important_score=None* is set by default, your function should return the average of all representation vectors corresponding to set *important_score=[1,1,...,1]*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF87Vg_C9jkD"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "def get_sentence_vector(sentence, important_score=None, model= None):\n",
    "    mysentence = sentence.split(' ')\n",
    "    L = len(mysentence)\n",
    "    final_vector= np.zeros((100,), dtype=\"float32\")\n",
    "    if important_score is None:\n",
    "        important_score = np.ones((L,), dtype=\"float32\")\n",
    "    else:\n",
    "        important_score = tf.nn.softmax(important_score)\n",
    "    \n",
    "    for i in range (L):\n",
    "        vector_i = get_word_vector(mysentence[i], model)\n",
    "        final_vector = final_vector + important_score[i]*vector_i\n",
    "    \n",
    "    return final_vector\n",
    "    #Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggz-CuHR9jkD"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.4**</span> \n",
    "\n",
    "**Write code to transform the training questions in *dm.train_str_questions* to feature vectors. Note that after running the following cell, you must have $X\\_train$ which is an numpy array of the feature vectors and $y\\_train$ which is an array of numeric labels (*Hint: dm.train_numeral_labels*). You can add more lines to the following cell if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2.5 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFXQgNmi9jkE",
    "outputId": "1f704278-fe06-49d5-e28e-d7038f5f8128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform training set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform training set to feature vectors...\")\n",
    "my_important_score = np.logspace(0,100,100,base=0.9)\n",
    "final_vector=list()\n",
    "for i in dm.train_str_questions:\n",
    "    final_vector.append( get_sentence_vector(i, important_score = None, model= word2vect) )\n",
    "X_train = np.array(final_vector,dtype=\"float32\")\n",
    "y_train = dm.train_numeral_labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPMJZgdM9jkE"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.5**</span> \n",
    "\n",
    "**Write code to transform the training questions in *dm.valid_str_questions* to feature vectors. Note that after running the following cell, you must have $X\\_valid$ which is an numpy array of the feature vectors and $y\\_valid$ which is an array of numeric labels (*Hint: dm.valid_numeral_labels*). You can add more lines to the following cell if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2.5 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaJLHJSX9jkE",
    "outputId": "1dabdddd-23f2-49cf-c624-ed6b8a242995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform valid set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform valid set to feature vectors...\")\n",
    "final_vector=list()\n",
    "for i in dm.valid_str_questions:\n",
    "    final_vector.append( get_sentence_vector(i, important_score = my_important_score, model= word2vect) )\n",
    "X_valid = np.array(final_vector,dtype=\"float32\")\n",
    "y_valid = dm.valid_numeral_labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I1UskcH9jkE"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.6**</span> \n",
    "\n",
    "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in sckit-learn to scale both training and valid sets to the range $(-1,1)$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfL5EUtY9jkE"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "# scaler.fit(X_train)\n",
    "# X_train = #Insert your code here\n",
    "# scaler.fit(X_valid)\n",
    "# X_valid = #Insert your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRdXKWBOkHNe",
    "outputId": "0610cbae-64d1-45a6-c3e1-2679887a317a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4W04n6GkmNw",
    "outputId": "adad0e62-385c-4dfd-e018-5965887bb893"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-eASJoN9jkE"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.7**</span> \n",
    "\n",
    "**Declare a support vector machine (the class *SVC*  in sckit-learn) with RBF kernel, $C=1$, $gamma= 2^{-3}$ and fit on the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[3 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-gWx4u99jkE",
    "outputId": "36b9dae0-c673-4b53-d795-3e1ff5763700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.125)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "mysvm = svm.SVC(C=1, kernel='rbf', gamma=1/8, decision_function_shape='ovr')\n",
    "mysvm.fit(X_train , y_train.ravel())\n",
    "#Insert your code for fitting svm on X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqPietln9jkE"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 2.8**</span> \n",
    "\n",
    "**Finally, we use the trained *svm* to evaluate on the valid set $X\\_valid$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsdwvyeN9jkF",
    "outputId": "98caf79b-62f1-4839-cb7d-f43d9064af4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8282828282828283\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred= mysvm.predict(X_valid)\n",
    "acc = accuracy_score(y_valid,y_valid_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUEZvG0K9jkF"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Text CNN for sequence modeling and neural embedding </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 15 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UchJj97b9jkF"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 3.1**</span> \n",
    "\n",
    "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN you need to construct.**\n",
    "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
    "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
    "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)).\n",
    "  - `data_manager`: the data manager to store information of the dataset.\n",
    "- The detail of the computational process is as follows:\n",
    "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size \\times vocab\\_size \\times embed\\_size]$ as $h$.\n",
    "  - We feed $h$ to three Convd 1D layers, each of which has $state\\_size$ filters, padding=same, activation= relu, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size \\times output\\_size \\times state\\_size]$.\n",
    "  - We then apply *GlobalMaxPool1D()* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
    "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$. Note that you need to specify the axis to concatenate.\n",
    "  - We finally build up one dense layer on the top of $h$ for classification.\n",
    "  \n",
    "  <div style=\"text-align: right\"><span style=\"color:red\">[10 points]</span></div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heKtSU04jBhx"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPool1D, Dense, Flatten, Concatenate, Embedding,Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CY2l1GLm9jkF"
   },
   "outputs": [],
   "source": [
    "from keras.layers.pooling import GlobalMaxPool1D\n",
    "class TextCNN:\n",
    "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
    "        self.data_manager = data_manager\n",
    "        self.embed_size = embed_size\n",
    "        self.state_size = state_size\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "\n",
    "        h = tf.keras.layers.Embedding(self.data_manager.vocab_size +1, self.embed_size)(x)\n",
    "        \n",
    "        h1 = Conv1D(filters=self.state_size, padding='same', activation= 'relu', kernel_size=3)(h)\n",
    "        h2 = Conv1D(filters=self.state_size, padding='same', activation= 'relu', kernel_size=5)(h)\n",
    "        h3 = Conv1D(filters=self.state_size, padding='same', activation= 'relu', kernel_size=7)(h)\n",
    "        \n",
    "        h1 = GlobalMaxPool1D(data_format='channels_last')(h1)\n",
    "        h2 = GlobalMaxPool1D(data_format='channels_last')(h2)\n",
    "        h3 = GlobalMaxPool1D(data_format='channels_last')(h3)\n",
    "        \n",
    "        h = Concatenate(axis=1)([h1,h2,h3])#Insert your code here\n",
    "        \n",
    "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h) \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZXKsJoK9jkF"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 3.2**</span> \n",
    "**Here is the code to test TextCNN above. You can observe that TextCNN outperforms the traditional approach SVM + Word2Vect for this task. The reason is that TextCNN enables us to automatically learn the feature that fits to the task. This makes deep learning different from hand-crafted feature approaches. Complete the code to test the model. Note that when compiling the model, you can use the Adam optimizer.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71AOUdSm9jkF",
    "outputId": "ce4aa436-cb83-433f-c323-566556cacd10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 2s 112ms/step - loss: 1.7824 - accuracy: 0.2145 - val_loss: 1.7284 - val_accuracy: 0.3838\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 1.6479 - accuracy: 0.4863 - val_loss: 1.6523 - val_accuracy: 0.4040\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 1.5327 - accuracy: 0.5112 - val_loss: 1.5645 - val_accuracy: 0.4141\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 1.4025 - accuracy: 0.5711 - val_loss: 1.4524 - val_accuracy: 0.4343\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 1.2526 - accuracy: 0.6334 - val_loss: 1.3254 - val_accuracy: 0.5051\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 1.0885 - accuracy: 0.6958 - val_loss: 1.1762 - val_accuracy: 0.5455\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.9134 - accuracy: 0.8554 - val_loss: 1.0173 - val_accuracy: 0.6667\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.7445 - accuracy: 0.9327 - val_loss: 0.8707 - val_accuracy: 0.7980\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.5968 - accuracy: 0.9551 - val_loss: 0.7480 - val_accuracy: 0.8081\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.4753 - accuracy: 0.9551 - val_loss: 0.6502 - val_accuracy: 0.8182\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.3789 - accuracy: 0.9676 - val_loss: 0.5722 - val_accuracy: 0.8586\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.3023 - accuracy: 0.9751 - val_loss: 0.5074 - val_accuracy: 0.8788\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.2416 - accuracy: 0.9850 - val_loss: 0.4531 - val_accuracy: 0.8788\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1938 - accuracy: 0.9850 - val_loss: 0.4063 - val_accuracy: 0.8788\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.1563 - accuracy: 0.9850 - val_loss: 0.3667 - val_accuracy: 0.9091\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.1268 - accuracy: 0.9850 - val_loss: 0.3320 - val_accuracy: 0.9192\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1037 - accuracy: 0.9925 - val_loss: 0.3027 - val_accuracy: 0.9293\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0855 - accuracy: 0.9950 - val_loss: 0.2780 - val_accuracy: 0.9293\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0710 - accuracy: 0.9950 - val_loss: 0.2575 - val_accuracy: 0.9394\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0595 - accuracy: 0.9975 - val_loss: 0.2404 - val_accuracy: 0.9495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc51b0e3d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cnn = TextCNN(data_manager=dm)\n",
    "text_cnn.build()\n",
    "text_cnn.compile_model(optimizer=\"adam\", loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),metrics=[\"accuracy\"])\n",
    "#Insert code here to compile the model\n",
    "text_cnn.model.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))\n",
    "#Insert code here to train the model on 20 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSGUGf4c9jkG"
   },
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: RNNs for sequence modeling and neural embedding </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 65 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p0YPGZj9jkG"
   },
   "source": [
    "### <span style=\"color:#0b486b\">4.1. One-directional RNNs for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 20 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhrScr5x9jkG"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.1**</span> \n",
    "**In this part, you need to construct an RNN to learn from the dataset of interest. Basically, you are required first to construct the class UniRNN (Uni-directional RNN) with the following requirements:**\n",
    "- Attribute `data_manager (self.data_manager)`: specifies the data manager used to store data for the model.\n",
    "- Attribute `cell_type (self.cell_type)`: can receive three values including `basic_rnn`, `gru`, and `lstm` which specifies the memory cells formed a hidden layer.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes from the second hidden layers of memory cells. For example, $embed\\_size =128$ and $state\\_sizes = [64, 64]$ means that you have three hidden layers in your network with hidden sizes of $128, 64$ and $64$ respectively.\n",
    "\n",
    "**Note that when declaring an embedding layer for the network, you need to set *mask_zero=True* so that the padding zeros in the sentences will be masked and ignored. This helps to have variable length RNNs. For more detail, you can refer to this [link](https://www.tensorflow.org/guide/keras/masking_and_padding).**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[10 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biRrZ1FCpXCO"
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU, LSTM, RNN, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aN2JaRPf9jkG"
   },
   "outputs": [],
   "source": [
    "class UniRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1 \n",
    "        \n",
    "    #return the correspoding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type== 'gru':\n",
    "            return GRU(state_size, return_sequences = return_sequences, activation = activation)#Insert your code here\n",
    "        elif cell_type== 'lstm':\n",
    "            return LSTM(state_size, return_sequences = return_sequences, activation = activation)#Insert your code here\n",
    "        else:\n",
    "            return RNN(state_size, return_sequences = return_sequences, activation = activation)#Insert your code here\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = Embedding(self.vocab_size, self.embed_size,mask_zero=True)(x)#Insert your code here\n",
    "        num_layers = len(self.state_sizes)#Insert your code here\n",
    "        for i in range(num_layers):\n",
    "            if (i ==  num_layers-1):\n",
    "                layer = self.get_layer(state_size=self.state_sizes[i])\n",
    "            else:\n",
    "                layer = self.get_layer(state_size=self.state_sizes[i],return_sequences= True)\n",
    "            h = layer(h) #Insert your code here, you can insert more lines if necessary\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "   \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "---stMdJ9jkG"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.2**</span> \n",
    "**Run with basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxmiLrSk9jkG",
    "outputId": "32e7fd11-ff49-4533-b115-711f1160a02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 14s 912ms/step - loss: 1.6249 - accuracy: 0.4015 - val_loss: 1.3798 - val_accuracy: 0.3939\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 3s 457ms/step - loss: 1.0835 - accuracy: 0.5436 - val_loss: 1.2462 - val_accuracy: 0.4646\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 3s 455ms/step - loss: 0.8501 - accuracy: 0.6359 - val_loss: 1.1352 - val_accuracy: 0.4545\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 4s 555ms/step - loss: 0.7123 - accuracy: 0.7406 - val_loss: 0.9301 - val_accuracy: 0.6162\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 3s 456ms/step - loss: 0.4899 - accuracy: 0.8603 - val_loss: 0.9394 - val_accuracy: 0.6162\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 3s 460ms/step - loss: 0.3236 - accuracy: 0.9227 - val_loss: 0.9735 - val_accuracy: 0.6162\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 4s 562ms/step - loss: 0.2150 - accuracy: 0.9651 - val_loss: 0.9789 - val_accuracy: 0.6364\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 3s 450ms/step - loss: 0.1285 - accuracy: 0.9776 - val_loss: 1.0301 - val_accuracy: 0.6566\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 3s 448ms/step - loss: 0.1179 - accuracy: 0.9776 - val_loss: 0.5141 - val_accuracy: 0.8384\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 3s 448ms/step - loss: 0.0584 - accuracy: 0.9900 - val_loss: 1.0910 - val_accuracy: 0.7273\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 3s 449ms/step - loss: 0.0395 - accuracy: 0.9975 - val_loss: 0.5393 - val_accuracy: 0.8485\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 3s 451ms/step - loss: 0.0221 - accuracy: 0.9975 - val_loss: 0.5729 - val_accuracy: 0.8586\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 3s 454ms/step - loss: 0.0155 - accuracy: 0.9975 - val_loss: 0.6621 - val_accuracy: 0.8485\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 3s 450ms/step - loss: 0.0114 - accuracy: 0.9975 - val_loss: 0.9258 - val_accuracy: 0.8182\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 3s 459ms/step - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.8380 - val_accuracy: 0.8182\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 3s 459ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.5142 - val_accuracy: 0.8687\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 3s 457ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5493 - val_accuracy: 0.8687\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 3s 453ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5806 - val_accuracy: 0.8788\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 3s 456ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 0.8788\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 3s 457ms/step - loss: 6.8759e-04 - accuracy: 1.0000 - val_loss: 0.6482 - val_accuracy: 0.8788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc51455a50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type= 'basic_rnn', embed_size= 128, state_sizes= [128, 128],data_manager=dm)#Insert your code here\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSO_GvNA9jkH"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.3**</span> \n",
    "**Run with GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAyEwLju9jkH",
    "outputId": "945010bb-531d-447a-d6b7-09f3f532b642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 14s 891ms/step - loss: 1.6514 - accuracy: 0.3691 - val_loss: 1.4606 - val_accuracy: 0.3636\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 3s 462ms/step - loss: 1.0850 - accuracy: 0.5736 - val_loss: 1.2296 - val_accuracy: 0.4949\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 3s 447ms/step - loss: 0.7497 - accuracy: 0.6908 - val_loss: 1.0291 - val_accuracy: 0.5253\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 3s 449ms/step - loss: 0.4824 - accuracy: 0.8404 - val_loss: 0.9286 - val_accuracy: 0.6465\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 3s 453ms/step - loss: 0.3171 - accuracy: 0.9027 - val_loss: 0.5813 - val_accuracy: 0.7677\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 3s 454ms/step - loss: 0.2061 - accuracy: 0.9451 - val_loss: 0.8587 - val_accuracy: 0.7677\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 3s 454ms/step - loss: 0.0974 - accuracy: 0.9925 - val_loss: 0.4569 - val_accuracy: 0.8283\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 3s 451ms/step - loss: 0.0560 - accuracy: 0.9950 - val_loss: 0.4927 - val_accuracy: 0.8384\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 3s 454ms/step - loss: 0.0356 - accuracy: 0.9975 - val_loss: 0.5929 - val_accuracy: 0.8283\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 3s 445ms/step - loss: 0.0272 - accuracy: 0.9975 - val_loss: 0.5101 - val_accuracy: 0.8687\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 3s 448ms/step - loss: 0.0391 - accuracy: 0.9900 - val_loss: 0.6975 - val_accuracy: 0.8384\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 3s 451ms/step - loss: 0.0305 - accuracy: 0.9950 - val_loss: 1.1064 - val_accuracy: 0.7879\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 3s 446ms/step - loss: 0.0656 - accuracy: 0.9800 - val_loss: 0.3566 - val_accuracy: 0.9091\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 3s 446ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3354 - val_accuracy: 0.8990\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 3s 452ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.3340 - val_accuracy: 0.8990\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 3s 449ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3517 - val_accuracy: 0.9091\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 3s 450ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 0.9091\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 3s 441ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.9091\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 3s 444ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4734 - val_accuracy: 0.8889\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 3s 446ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5805 - val_accuracy: 0.8788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc5bc11c10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type= 'gru', embed_size= 128, state_sizes= [128, 128],data_manager=dm) #Insert your code here\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0MQ8cIm9jkH"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.4**</span> \n",
    "**Run with LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LRigkgu9jkH",
    "outputId": "7afc8bbe-cb47-4233-a160-92c9f05dac0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 15s 1s/step - loss: 1.6542 - accuracy: 0.3791 - val_loss: 1.4446 - val_accuracy: 0.3939\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 3s 454ms/step - loss: 1.0775 - accuracy: 0.5711 - val_loss: 1.1594 - val_accuracy: 0.4949\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 3s 454ms/step - loss: 0.7064 - accuracy: 0.6983 - val_loss: 0.9575 - val_accuracy: 0.5758\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 3s 452ms/step - loss: 0.4259 - accuracy: 0.8454 - val_loss: 0.8463 - val_accuracy: 0.6768\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 3s 452ms/step - loss: 0.3374 - accuracy: 0.8878 - val_loss: 0.5136 - val_accuracy: 0.7879\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 3s 451ms/step - loss: 0.1379 - accuracy: 0.9825 - val_loss: 0.6216 - val_accuracy: 0.7980\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 3s 453ms/step - loss: 0.0749 - accuracy: 0.9875 - val_loss: 0.5014 - val_accuracy: 0.8485\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 3s 460ms/step - loss: 0.0489 - accuracy: 0.9950 - val_loss: 0.9249 - val_accuracy: 0.7677\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 3s 456ms/step - loss: 0.0507 - accuracy: 0.9900 - val_loss: 0.4315 - val_accuracy: 0.8990\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 3s 450ms/step - loss: 0.0411 - accuracy: 0.9950 - val_loss: 0.4053 - val_accuracy: 0.9091\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 3s 448ms/step - loss: 0.0229 - accuracy: 0.9950 - val_loss: 0.4501 - val_accuracy: 0.8990\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 3s 460ms/step - loss: 0.0307 - accuracy: 0.9950 - val_loss: 0.3688 - val_accuracy: 0.9091\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 3s 457ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.3519 - val_accuracy: 0.9091\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 3s 455ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.9091\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 3s 459ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.4647 - val_accuracy: 0.8990\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 3s 461ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.9355 - val_accuracy: 0.8384\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 3s 460ms/step - loss: 0.0277 - accuracy: 0.9975 - val_loss: 0.8426 - val_accuracy: 0.8283\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 3s 455ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4532 - val_accuracy: 0.8990\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 3s 450ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4731 - val_accuracy: 0.8990\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 3s 450ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4578 - val_accuracy: 0.8990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc4f4dca10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type= 'lstm', embed_size= 128, state_sizes= [128, 128],data_manager=dm)#Insert your code here\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xOT2KfJ9jkH"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.5**</span> \n",
    "**Give your own comments about the performance of three memory cells for the dataset of interest as well as what happening during the training process of each cell. Note that there are not right or wrong comments and your comments rely on the status of your training. In addition, some comments and hypothesized assessments of what and why are occurring are useful to obtain the highest score for this question.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hysxzy6f9jkI"
   },
   "source": [
    "#Your comments here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmBoNA6xO2oe"
   },
   "source": [
    "All three models get a good score of accuracy= 1.0000 in the training set\n",
    "\n",
    "basic_rnn gets accuracy= 1.0000 at 17 epochs\n",
    "gru gets accuracy= 1.0000 at 14 epochs\n",
    "lstm gets accuracy= 1.0000 at epoch 13\n",
    "\n",
    "On the validation set basic_rnn: val_accuracy=0.8788, gru: val_accuracy=0.8788, lstm: val_accuracy: 0.8990\n",
    "The final result obtained by lstm is better than basic_rnn, gru\n",
    "LSTM is an excellent variant model of RNN. It inherits the characteristics of the RNN model and converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ5lqwd79jkI"
   },
   "source": [
    "### <span style=\"color:#0b486b\">4.2. Bi-directional RNNs for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 20 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L01rCJjz9jkI"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.1**</span> \n",
    "**In what follow, you will investigate BiRNN. The task is similar to Part 4.1 but you need to write the code for an BiRNN. Note that the function *get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh')* has to return the hidden layer with bidirectional memory cells (e.g., Basic RNN, GRU, and LSTM cells).**\n",
    "\n",
    "**Complete the code of the class *BiRNN*. Note that for the embedding layer you need to set *mask_zero=True*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[10 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2yfWFKJ9jkI"
   },
   "outputs": [],
   "source": [
    "class BiRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type=='gru':\n",
    "            return Bidirectional(GRU(state_size,return_sequences= return_sequences, activation = activation))#Insert your code here\n",
    "        elif cell_type== 'lstm':\n",
    "            return Bidirectional(LSTM(state_size,return_sequences= return_sequences, activation = activation))#Insert your code here\n",
    "        else:\n",
    "            return Bidirectional(RNN(state_size,return_sequences= return_sequences, activation = activation))#Insert your code here\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = Embedding(self.vocab_size, self.embed_size,mask_zero=True)(x)#Insert your code here\n",
    "        num_layers = len(self.state_sizes)#Insert your code here\n",
    "        for i in range(num_layers):\n",
    "            if (i ==  num_layers-1):\n",
    "                layer = self.get_layer(state_size=self.state_sizes[i])\n",
    "            else:\n",
    "                layer = self.get_layer(state_size=self.state_sizes[i],return_sequences= True)\n",
    "            h = layer(h)#Insert your code here, add more lines if necessary\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzazq3lE9jkI"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.2**</span> \n",
    "**Run BiRNN for basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DhVAeXw9jkI",
    "outputId": "e8bb929d-8e59-447c-86c5-a6cb1c87f28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 30s 2s/step - loss: 1.5684 - accuracy: 0.4364 - val_loss: 1.2475 - val_accuracy: 0.4242\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 9s 1s/step - loss: 0.8800 - accuracy: 0.6733 - val_loss: 1.0802 - val_accuracy: 0.5253\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.5490 - accuracy: 0.8055 - val_loss: 0.6421 - val_accuracy: 0.7071\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.2822 - accuracy: 0.9277 - val_loss: 0.5116 - val_accuracy: 0.8182\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1207 - accuracy: 0.9800 - val_loss: 0.3898 - val_accuracy: 0.8687\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0586 - accuracy: 0.9825 - val_loss: 0.3609 - val_accuracy: 0.8586\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0324 - accuracy: 0.9900 - val_loss: 0.2139 - val_accuracy: 0.9495\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.2101 - val_accuracy: 0.9596\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2219 - val_accuracy: 0.9596\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9596\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2561 - val_accuracy: 0.9495\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 8.9728e-04 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 0.9495\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 5.1290e-04 - accuracy: 1.0000 - val_loss: 0.2977 - val_accuracy: 0.9495\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 3.0977e-04 - accuracy: 1.0000 - val_loss: 0.3163 - val_accuracy: 0.9495\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.9469e-04 - accuracy: 1.0000 - val_loss: 0.3337 - val_accuracy: 0.9495\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 9s 1s/step - loss: 1.2589e-04 - accuracy: 1.0000 - val_loss: 0.3504 - val_accuracy: 0.9495\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 8.3133e-05 - accuracy: 1.0000 - val_loss: 0.3667 - val_accuracy: 0.9495\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 5.5796e-05 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 0.9394\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 3.7933e-05 - accuracy: 1.0000 - val_loss: 0.3984 - val_accuracy: 0.9293\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 2.6069e-05 - accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.9293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc482d03d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type= 'bi_rnn', embed_size= 128, state_sizes= [128, 128],data_manager=dm)#Insert your code here\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej8g-L_c9jkJ"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.3**</span> \n",
    "**Run BiRNN for GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhRFu3-y9jkJ",
    "outputId": "9ba3b0f1-678d-433f-8841-f9e765ed8faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 30s 2s/step - loss: 1.5729 - accuracy: 0.4190 - val_loss: 1.2782 - val_accuracy: 0.4040\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.9297 - accuracy: 0.6459 - val_loss: 0.9584 - val_accuracy: 0.5253\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.5009 - accuracy: 0.8429 - val_loss: 0.8392 - val_accuracy: 0.6869\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.2430 - accuracy: 0.9352 - val_loss: 0.7231 - val_accuracy: 0.7273\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 14s 2s/step - loss: 0.1000 - accuracy: 0.9776 - val_loss: 0.6029 - val_accuracy: 0.7980\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0490 - accuracy: 0.9825 - val_loss: 0.3835 - val_accuracy: 0.8687\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 0.4173 - val_accuracy: 0.8990\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9192\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 0.9192\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3962 - val_accuracy: 0.9091\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4226 - val_accuracy: 0.9091\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 9.1305e-04 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.8990\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 5.4328e-04 - accuracy: 1.0000 - val_loss: 0.4783 - val_accuracy: 0.8990\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 3.3543e-04 - accuracy: 1.0000 - val_loss: 0.5050 - val_accuracy: 0.8889\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.1302e-04 - accuracy: 1.0000 - val_loss: 0.5303 - val_accuracy: 0.8889\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 1.3816e-04 - accuracy: 1.0000 - val_loss: 0.5542 - val_accuracy: 0.8889\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 9.1080e-05 - accuracy: 1.0000 - val_loss: 0.5769 - val_accuracy: 0.8990\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 9s 1s/step - loss: 6.0861e-05 - accuracy: 1.0000 - val_loss: 0.5984 - val_accuracy: 0.8990\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 4.1158e-05 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 0.8990\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 2.8134e-05 - accuracy: 1.0000 - val_loss: 0.6384 - val_accuracy: 0.8990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc446af390>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type= 'gru', embed_size= 128, state_sizes= [128, 128],data_manager=dm)#Insert your code here\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOmgM6sm9jkJ"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.4**</span> \n",
    "**Run BiRNN for LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCs8UYsr9jkJ",
    "outputId": "b6fd3545-49de-408b-c5da-b7e7d5c90d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 33s 2s/step - loss: 1.5703 - accuracy: 0.4464 - val_loss: 1.3448 - val_accuracy: 0.4242\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.9018 - accuracy: 0.6534 - val_loss: 0.9249 - val_accuracy: 0.5455\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.5214 - accuracy: 0.8354 - val_loss: 0.7491 - val_accuracy: 0.6869\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.2697 - accuracy: 0.9277 - val_loss: 0.5973 - val_accuracy: 0.7879\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.2272 - accuracy: 0.9202 - val_loss: 0.9126 - val_accuracy: 0.6869\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0943 - accuracy: 0.9676 - val_loss: 0.5625 - val_accuracy: 0.8081\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 12s 2s/step - loss: 0.0501 - accuracy: 0.9875 - val_loss: 0.3550 - val_accuracy: 0.8889\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0314 - accuracy: 0.9925 - val_loss: 0.3325 - val_accuracy: 0.9091\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0181 - accuracy: 0.9975 - val_loss: 0.3720 - val_accuracy: 0.9091\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3850 - val_accuracy: 0.9192\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4199 - val_accuracy: 0.9192\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 0.9091\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4749 - val_accuracy: 0.8990\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 7.0932e-04 - accuracy: 1.0000 - val_loss: 0.5013 - val_accuracy: 0.8990\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 4.2500e-04 - accuracy: 1.0000 - val_loss: 0.5278 - val_accuracy: 0.8788\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 14s 2s/step - loss: 2.6219e-04 - accuracy: 1.0000 - val_loss: 0.5542 - val_accuracy: 0.8788\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 1.6568e-04 - accuracy: 1.0000 - val_loss: 0.5796 - val_accuracy: 0.8788\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 1.0680e-04 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 0.8788\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 7s 1s/step - loss: 6.9950e-05 - accuracy: 1.0000 - val_loss: 0.6292 - val_accuracy: 0.8788\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 9s 1s/step - loss: 4.6444e-05 - accuracy: 1.0000 - val_loss: 0.6537 - val_accuracy: 0.8788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc4096e850>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type= 'lstm', embed_size= 128, state_sizes= [128, 128],data_manager=dm)#Insert your code here\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E50FkAom9jkJ"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.5**</span> \n",
    "\n",
    "**Give your own comments about the performance of three memory cells for the dataset of interest as well as comparing BiRNN to UniRNN in Part 1.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRndDDKb9jkK"
   },
   "source": [
    "#Your comments here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD_MiP6peJJ7"
   },
   "source": [
    "#Your comments here\n",
    "Similar to UniRNN, all three models got accuracy= 1.0000 after BiRNN training\n",
    "\n",
    "\n",
    "\n",
    "basic_rnn gets accuracy= 1.0000 at 8 epochs\n",
    "gru gets accuracy= 1.0000 at 8 epochs\n",
    "lstm gets accuracy= 1.0000 at the 9th epoch\n",
    "\n",
    "On validation set basic_rnn: val_accuracy=0.9293, gru: val_accuracy= 0.8990, lstm: val_accuracy= 0.8788\n",
    "\n",
    "The result of basic_rnn in BiRNN is better than lstm, gru\n",
    "lstm is not as good as basic_rnn, gru in BiRNN, slower convergence, lowest val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ2MTpgK9jkK"
   },
   "source": [
    "### <span style=\"color:#0b486b\">4.3. RNNs with various types, cells, and fine-tuning embedding matrix for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 20 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7EbsXse9jkK"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.3.1**</span> \n",
    "\n",
    "**In what follows, you are required to combine the code in Part 1 and Part 2 to gain a general RNN which can be either Uni-directional RNN or Bi-directional RNN and the embedding matrix can be initialized using a pretrained Word2Vect.**\n",
    "\n",
    "**Below are the descriptions of the attributes of the class *RNN*:**\n",
    "- `run_mode (self.run_mode)` has three values (scratch, init-only, and init-fine-tune).\n",
    "  - `scratch` means training the embedding matrix from scratch.\n",
    "  - `init-only` means only initialzing the embedding matrix with a pretrained Word2Vect but not further doing fine-tuning that matrix.\n",
    "  - `init-fine-tune` means both initialzing the embedding matrix with a pretrained Word2Vect and further doing fine-tuning that matrix.\n",
    "- `network_type (self.network_type)` has two values (uni-directional and bi-directional) which correspond to either Uni-directional RNN or Bi-directional RNN.\n",
    "- `cell_type (self.cell_type)` has three values (simple-rnn, gru, and lstm) which specify the memory cell used in the network.\n",
    "- `embed_model (self.embed_model)` specifes the pretrained Word2Vect model used.\n",
    "-  `embed_size (self.embed_size)` specifes the embedding size. Note that when run_mode is either init-only' or 'init-fine-tune', this embedding size is extracted from embed_model for dimension compatability.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes from the second hidden layers of memory cells. For example, $embed\\_size =128$ and $state\\_sizes = [64, 64]$ means that you have three hidden layers in your network with hidden sizes of $128, 64$ and $64$ respectively.\n",
    "\n",
    "**Complete the code of the class *RNN*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[10 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ltYho4ye9jkK"
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, run_mode = 'scratch', cell_type= 'gru', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', \n",
    "                 embed_size= 128, state_sizes = [64, 64], data_manager = None):\n",
    "        self.run_mode = run_mode\n",
    "        self.data_manager = data_manager\n",
    "        self.cell_type = cell_type\n",
    "        self.network_type = network_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_model = embed_model\n",
    "        self.embed_size = embed_size\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = dm.vocab_size +1\n",
    "        self.word2idx = dm.word2idx\n",
    "        self.word2vect = None\n",
    "        self.embed_matrix = np.zeros(shape= [self.vocab_size, self.embed_size])\n",
    "    \n",
    "    def build_embedding_matrix(self):\n",
    "        self.word2vect = api.load(self.embed_model)#Insert your code here\n",
    "        for word, idx in self.word2idx.items():\n",
    "            try:\n",
    "                self.embed_matrix[idx] = self.word2vect.word_vec(word)\n",
    "            except KeyError:\n",
    "                pass     \n",
    "        #Insert your code here\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', network_type= 'uni-directional', hidden_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if (network_type == 'uni-directional'):\n",
    "            if cell_type== 'gru':\n",
    "                return GRU(hidden_size, return_sequences = return_sequences, activation = activation)#Insert your code here\n",
    "            elif cell_type== 'lstm':\n",
    "                return LSTM(hidden_size, return_sequences = return_sequences, activation = activation)#Insert your code here\n",
    "            else:\n",
    "                return RNN(hidden_size, return_sequences = return_sequences, activation = activation)#Insert your code here\n",
    "    \n",
    "\n",
    "        else:\n",
    "            if cell_type=='gru':\n",
    "                return Bidirectional(GRU(hidden_size,return_sequences= return_sequences, activation = activation))#Insert your code here\n",
    "            elif cell_type== 'lstm':\n",
    "                return Bidirectional(LSTM(hidden_size,return_sequences= return_sequences, activation = activation))#Insert your code here\n",
    "            else:\n",
    "                return Bidirectional(RNN(hidden_size,return_sequences= return_sequences, activation = activation))#Insert your code here\n",
    "    #Insert your code here\n",
    "    \n",
    "    def build(self):\n",
    "        x = Input(shape=[None])\n",
    "        if self.run_mode == \"scratch\":\n",
    "            self.embedding_layer = Embedding(self.vocab_size, self.embed_size, trainable= True,mask_zero=True)\n",
    "        elif self.run_mode == \"init-only\":\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = Embedding(self.vocab_size, self.embed_size, weights= [self.embed_matrix], trainable= False,mask_zero=True) \n",
    "        else : #fine-tuned\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = Embedding(self.vocab_size, self.embed_size, weights= [self.embed_matrix], trainable= True,mask_zero=True) \n",
    "        h = self.embedding_layer(x)\n",
    "        \n",
    "        num_layers = len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            if (i == num_layers-1):\n",
    "                layer = self.get_layer(hidden_size=self.state_sizes[i])\n",
    "            else:\n",
    "                layer = self.get_layer(hidden_size=self.state_sizes[i],return_sequences= True)\n",
    "                \n",
    "        h=layer(h)                                                     \n",
    "           \n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "        \n",
    "        #Insert your code here\n",
    "        \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USg_RN3b9jkK"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.3.2**</span> \n",
    "\n",
    "**Design the experiment to compare three running modes. Note that you should stick with fixed values for other attributes and only vary *run_mode*. Give your comments for the results.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoOGN5E89jkK",
    "outputId": "182c4e7e-d41a-400c-b8af-90fa2b7ee8ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running mode:  scratch\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m modes:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning mode: \u001b[39m\u001b[38;5;124m\"\u001b[39m,mode)\n\u001b[1;32m----> 5\u001b[0m     rnn \u001b[38;5;241m=\u001b[39m RNN(data_manager\u001b[38;5;241m=\u001b[39m\u001b[43mdm\u001b[49m, run_mode\u001b[38;5;241m=\u001b[39m mode,cell_type\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm\u001b[39m\u001b[38;5;124m'\u001b[39m, embed_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, state_sizes\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m])\n\u001b[0;32m      6\u001b[0m     rnn\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m      8\u001b[0m     opt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dm' is not defined"
     ]
    }
   ],
   "source": [
    "modes = [\"scratch\",\"init-only\",\"init-fine-tune\"]\n",
    "for mode in modes:\n",
    "    print(\"running mode: \",mode)\n",
    "\n",
    "    rnn = RNN(data_manager=dm, run_mode= mode,cell_type= 'lstm', embed_size= 128, state_sizes= [128, 128])\n",
    "    rnn.build()\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB0C0mNi9jkK"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.3.3**</span> \n",
    "\n",
    "**Run the above general RNN with at least five parameter sets and try to obtain the best performance. You can stick with the running mode *init-fine-tune* and use grid search to tune other parameters. Record your best model which will be used in the next part.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3q4RdjRRfLsp",
    "outputId": "871424b7-0769-4817-ba99-26d6bedb9436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running cell_type:  simple-rnn\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 15s 580ms/step - loss: 1.5361 - accuracy: 0.3766 - val_loss: 1.3977 - val_accuracy: 0.4444\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 1.1246 - accuracy: 0.5661 - val_loss: 1.1724 - val_accuracy: 0.4747\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.9366 - accuracy: 0.6608 - val_loss: 1.0545 - val_accuracy: 0.5152\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 197ms/step - loss: 0.7919 - accuracy: 0.7531 - val_loss: 0.9352 - val_accuracy: 0.5859\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.6345 - accuracy: 0.8180 - val_loss: 0.7699 - val_accuracy: 0.6869\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 199ms/step - loss: 0.4647 - accuracy: 0.8678 - val_loss: 0.5891 - val_accuracy: 0.7980\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.3171 - accuracy: 0.9152 - val_loss: 0.4525 - val_accuracy: 0.8384\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.2101 - accuracy: 0.9476 - val_loss: 0.3451 - val_accuracy: 0.8586\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.1392 - accuracy: 0.9676 - val_loss: 0.2757 - val_accuracy: 0.8990\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 3s 446ms/step - loss: 0.1089 - accuracy: 0.9776 - val_loss: 0.2679 - val_accuracy: 0.9091\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 3s 422ms/step - loss: 0.0634 - accuracy: 0.9900 - val_loss: 0.2673 - val_accuracy: 0.9192\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0420 - accuracy: 0.9950 - val_loss: 0.2041 - val_accuracy: 0.9394\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 2s 291ms/step - loss: 0.0350 - accuracy: 0.9975 - val_loss: 0.1838 - val_accuracy: 0.9293\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 2s 285ms/step - loss: 0.0672 - accuracy: 0.9800 - val_loss: 0.1573 - val_accuracy: 0.9596\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 2s 198ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 0.9697\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9798\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.1316 - val_accuracy: 0.9697\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9697\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1455 - val_accuracy: 0.9495\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 199ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1782 - val_accuracy: 0.9394\n",
      "running cell_type:  gru\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 7s 431ms/step - loss: 1.5350 - accuracy: 0.3990 - val_loss: 1.4123 - val_accuracy: 0.3838\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 1.1067 - accuracy: 0.5761 - val_loss: 1.1812 - val_accuracy: 0.4242\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.9117 - accuracy: 0.6858 - val_loss: 1.0481 - val_accuracy: 0.5051\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 212ms/step - loss: 0.7473 - accuracy: 0.7556 - val_loss: 0.9259 - val_accuracy: 0.5960\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 207ms/step - loss: 0.5705 - accuracy: 0.8354 - val_loss: 0.7827 - val_accuracy: 0.6566\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.4114 - accuracy: 0.8853 - val_loss: 0.5945 - val_accuracy: 0.7576\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.2844 - accuracy: 0.9202 - val_loss: 0.6686 - val_accuracy: 0.7475\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.1765 - accuracy: 0.9576 - val_loss: 0.4329 - val_accuracy: 0.8586\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.1560 - accuracy: 0.9451 - val_loss: 0.4619 - val_accuracy: 0.8384\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0809 - accuracy: 0.9875 - val_loss: 0.3800 - val_accuracy: 0.8687\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0556 - accuracy: 0.9925 - val_loss: 0.3520 - val_accuracy: 0.8788\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0388 - accuracy: 0.9950 - val_loss: 0.3117 - val_accuracy: 0.8889\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0257 - accuracy: 0.9975 - val_loss: 0.3476 - val_accuracy: 0.8990\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.7021 - val_accuracy: 0.8283\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0867 - accuracy: 0.9726 - val_loss: 0.2498 - val_accuracy: 0.9293\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.2591 - val_accuracy: 0.9293\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 208ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2759 - val_accuracy: 0.9293\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.3263 - val_accuracy: 0.9192\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 207ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 0.8990\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 208ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2293 - val_accuracy: 0.9192\n",
      "running cell_type:  lstm\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 7s 429ms/step - loss: 1.5036 - accuracy: 0.4214 - val_loss: 1.4020 - val_accuracy: 0.3939\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 1.0930 - accuracy: 0.6234 - val_loss: 1.1407 - val_accuracy: 0.4343\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.8593 - accuracy: 0.7207 - val_loss: 0.9670 - val_accuracy: 0.5354\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 199ms/step - loss: 0.6669 - accuracy: 0.7980 - val_loss: 0.8083 - val_accuracy: 0.6162\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.5177 - accuracy: 0.8429 - val_loss: 0.6769 - val_accuracy: 0.7172\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 209ms/step - loss: 0.3926 - accuracy: 0.8828 - val_loss: 0.5658 - val_accuracy: 0.7980\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 0.2759 - accuracy: 0.9252 - val_loss: 0.4739 - val_accuracy: 0.8384\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.1866 - accuracy: 0.9526 - val_loss: 0.3970 - val_accuracy: 0.8384\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.1195 - accuracy: 0.9776 - val_loss: 0.3219 - val_accuracy: 0.8889\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0722 - accuracy: 0.9900 - val_loss: 0.3158 - val_accuracy: 0.8788\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0834 - accuracy: 0.9800 - val_loss: 0.2751 - val_accuracy: 0.8687\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.2036 - val_accuracy: 0.8889\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.2177 - val_accuracy: 0.9091\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 208ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.1977 - val_accuracy: 0.9192\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.5233 - val_accuracy: 0.8384\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0834 - accuracy: 0.9701 - val_loss: 0.1283 - val_accuracy: 0.9697\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9798\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9697\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 0.9697\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9596\n",
      "running cell_type:  simple-rnn\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 7s 426ms/step - loss: 1.4913 - accuracy: 0.4364 - val_loss: 1.3701 - val_accuracy: 0.4141\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 1.0817 - accuracy: 0.5935 - val_loss: 1.1305 - val_accuracy: 0.4747\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.8777 - accuracy: 0.6908 - val_loss: 0.9869 - val_accuracy: 0.5859\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.7010 - accuracy: 0.7706 - val_loss: 0.8339 - val_accuracy: 0.6162\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.5387 - accuracy: 0.8304 - val_loss: 0.7202 - val_accuracy: 0.7172\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.4054 - accuracy: 0.8828 - val_loss: 0.6486 - val_accuracy: 0.7677\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.2939 - accuracy: 0.9177 - val_loss: 0.5689 - val_accuracy: 0.7475\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.2106 - accuracy: 0.9352 - val_loss: 0.4828 - val_accuracy: 0.7879\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.1479 - accuracy: 0.9601 - val_loss: 0.4180 - val_accuracy: 0.8283\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.0996 - accuracy: 0.9776 - val_loss: 0.3633 - val_accuracy: 0.8586\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.0668 - accuracy: 0.9900 - val_loss: 0.3398 - val_accuracy: 0.8889\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 3s 379ms/step - loss: 0.0545 - accuracy: 0.9900 - val_loss: 0.3721 - val_accuracy: 0.8485\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 2s 200ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.2767 - val_accuracy: 0.9091\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.8283\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0344 - accuracy: 0.9950 - val_loss: 0.2476 - val_accuracy: 0.9192\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0236 - accuracy: 0.9975 - val_loss: 0.2432 - val_accuracy: 0.9293\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.3006 - val_accuracy: 0.9091\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.8990\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 198ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9192\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.2072 - val_accuracy: 0.9394\n",
      "running cell_type:  gru\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 7s 427ms/step - loss: 1.4800 - accuracy: 0.4289 - val_loss: 1.3756 - val_accuracy: 0.4141\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 1.0792 - accuracy: 0.6010 - val_loss: 1.1527 - val_accuracy: 0.4747\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.8872 - accuracy: 0.6858 - val_loss: 1.0161 - val_accuracy: 0.5455\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.7147 - accuracy: 0.7980 - val_loss: 0.8715 - val_accuracy: 0.5758\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.5387 - accuracy: 0.8603 - val_loss: 0.7339 - val_accuracy: 0.6667\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.3990 - accuracy: 0.8928 - val_loss: 0.6066 - val_accuracy: 0.7071\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.2899 - accuracy: 0.9127 - val_loss: 0.4883 - val_accuracy: 0.7980\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.1962 - accuracy: 0.9401 - val_loss: 0.3917 - val_accuracy: 0.8586\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.1190 - accuracy: 0.9701 - val_loss: 0.2800 - val_accuracy: 0.9192\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.0837 - accuracy: 0.9800 - val_loss: 0.2313 - val_accuracy: 0.9293\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0484 - accuracy: 0.9950 - val_loss: 0.2028 - val_accuracy: 0.9293\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 3s 436ms/step - loss: 0.0286 - accuracy: 0.9975 - val_loss: 0.1915 - val_accuracy: 0.9495\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0564 - accuracy: 0.9850 - val_loss: 0.2114 - val_accuracy: 0.9192\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0223 - accuracy: 0.9975 - val_loss: 0.1538 - val_accuracy: 0.9495\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 211ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9495\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.9495\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9495\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 205ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9394\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1625 - val_accuracy: 0.9495\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1730 - val_accuracy: 0.9394\n",
      "running cell_type:  lstm\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 8s 425ms/step - loss: 1.5411 - accuracy: 0.4040 - val_loss: 1.4571 - val_accuracy: 0.3838\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 1.1196 - accuracy: 0.6035 - val_loss: 1.2310 - val_accuracy: 0.4141\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 200ms/step - loss: 0.9141 - accuracy: 0.6833 - val_loss: 1.0669 - val_accuracy: 0.4545\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.7298 - accuracy: 0.7756 - val_loss: 0.8676 - val_accuracy: 0.5960\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.5445 - accuracy: 0.8304 - val_loss: 0.6745 - val_accuracy: 0.6869\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.4020 - accuracy: 0.8753 - val_loss: 0.5447 - val_accuracy: 0.8283\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.2834 - accuracy: 0.9227 - val_loss: 0.4952 - val_accuracy: 0.8384\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 207ms/step - loss: 0.2105 - accuracy: 0.9401 - val_loss: 0.4440 - val_accuracy: 0.8586\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.1413 - accuracy: 0.9676 - val_loss: 0.3472 - val_accuracy: 0.8990\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 202ms/step - loss: 0.1012 - accuracy: 0.9776 - val_loss: 0.2839 - val_accuracy: 0.8990\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 201ms/step - loss: 0.0737 - accuracy: 0.9825 - val_loss: 0.2628 - val_accuracy: 0.8889\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0413 - accuracy: 0.9950 - val_loss: 0.2220 - val_accuracy: 0.9091\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 207ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9091\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0712 - accuracy: 0.9726 - val_loss: 0.5307 - val_accuracy: 0.8485\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0322 - accuracy: 0.9900 - val_loss: 0.1617 - val_accuracy: 0.9596\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 203ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.1481 - val_accuracy: 0.9495\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 206ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9495\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1438 - val_accuracy: 0.9697\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 204ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.1466 - val_accuracy: 0.9697\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 207ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1515 - val_accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "cell_types=['simple-rnn', 'gru', 'lstm']\n",
    "network_type = ['bi-directional', 'uni-directional']\n",
    "for nettype in network_type:\n",
    "    for cell in cell_types:\n",
    "        print(\"running cell_type: \",cell)\n",
    "        print(\"running network_typ: \",nettype)\n",
    "        rnn = RNN(data_manager=dm, run_mode= 'init-fine-tune', cell_type=cell , network_type = nettype ,embed_size= 128, state_sizes= [128, 128])\n",
    "        rnn.build()\n",
    "    \n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "        rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mea2fTzH9jkL"
   },
   "source": [
    "#Report your results here\n",
    "\n",
    "Model 1 (run_mode ='init-fine-tune',...): accuracy = ...\n",
    "\n",
    "......................................................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZZHpXNbeqFO"
   },
   "source": [
    "Model 1 (run_mode ='init-fine-tune',network_type = 'bi-directional',cell_type= 'simple-rnn', embed_model= 'glove-wiki-gigaword-100',embed_size= 128, state_sizes = [64, 64], data_manager = dm): accuracy = 0.9394\n",
    "\n",
    "Model 2 (run_mode ='init-fine-tune',network_type = 'bi-directional',cell_type= 'gru', embed_model= 'glove-wiki-gigaword-100',embed_size= 128, state_sizes = [64, 64], data_manager = dm): accuracy = 0.9192\n",
    "\n",
    "Model 3 (run_mode ='init-fine-tune',network_type = 'bi-directional',cell_type= 'lstm', embed_model= 'glove-wiki-gigaword-100',embed_size= 128, state_sizes = [64, 64], data_manager = dm): accuracy = 0.9596\n",
    "\n",
    "\n",
    "Model 4 (run_mode ='init-fine-tune',network_type = 'uni-directional',cell_type= 'simple-rnn', embed_model= 'glove-wiki-gigaword-100',embed_size= 128, state_sizes = [64, 64], data_manager = dm): accuracy = 0.9394\n",
    "\n",
    "Model 5 (run_mode ='init-fine-tune',network_type = 'uni-directional',cell_type= 'gru', embed_model= 'glove-wiki-gigaword-100',embed_size= 128, state_sizes = [64, 64], data_manager = dm): accuracy = 0.9394\n",
    "\n",
    "Model 6 (run_mode ='init-fine-tune',network_type = 'uni-directional',cell_type= 'lstm', embed_model= 'glove-wiki-gigaword-100',embed_size= 128, state_sizes = [64, 64], data_manager = dm): accuracy = 0.9697\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoP7FtuB9jkL",
    "outputId": "87fd1c63-05d3-4798-ad65-f25648cb7329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 6s 324ms/step - loss: 1.6463 - accuracy: 0.3441 - val_loss: 1.4959 - val_accuracy: 0.3737\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 1.3354 - accuracy: 0.4838 - val_loss: 1.3267 - val_accuracy: 0.4242\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 1.1613 - accuracy: 0.5436 - val_loss: 1.1980 - val_accuracy: 0.4646\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1.0267 - accuracy: 0.6259 - val_loss: 1.1001 - val_accuracy: 0.5253\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 0.9152 - accuracy: 0.6858 - val_loss: 1.0218 - val_accuracy: 0.5859\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.8117 - accuracy: 0.7431 - val_loss: 0.9498 - val_accuracy: 0.6364\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.7075 - accuracy: 0.8055 - val_loss: 0.8762 - val_accuracy: 0.6566\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.6019 - accuracy: 0.8504 - val_loss: 0.7977 - val_accuracy: 0.6869\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.4992 - accuracy: 0.8828 - val_loss: 0.7140 - val_accuracy: 0.6869\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.4034 - accuracy: 0.9102 - val_loss: 0.6272 - val_accuracy: 0.7677\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.3173 - accuracy: 0.9302 - val_loss: 0.5413 - val_accuracy: 0.8182\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 0.2434 - accuracy: 0.9451 - val_loss: 0.4638 - val_accuracy: 0.8586\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.1826 - accuracy: 0.9601 - val_loss: 0.3991 - val_accuracy: 0.8586\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.1346 - accuracy: 0.9776 - val_loss: 0.3466 - val_accuracy: 0.8687\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 121ms/step - loss: 0.0983 - accuracy: 0.9900 - val_loss: 0.3056 - val_accuracy: 0.8990\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.0713 - accuracy: 0.9950 - val_loss: 0.2697 - val_accuracy: 0.8990\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.0508 - accuracy: 0.9950 - val_loss: 0.2463 - val_accuracy: 0.9293\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.0367 - accuracy: 0.9950 - val_loss: 0.2276 - val_accuracy: 0.9394\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.0351 - accuracy: 0.9950 - val_loss: 0.2330 - val_accuracy: 0.9293\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc3728cdd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The run of your best model here\n",
    "#Insert your code here\n",
    "\n",
    "my_best_rnn = RNN(run_mode ='init-fine-tune',network_type = 'uni-directional',cell_type= 'lstm', embed_model= 'glove-wiki-gigaword-100',embed_size= 128, state_sizes = [64, 64], data_manager = dm)\n",
    "my_best_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "my_best_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "my_best_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_Aq9zSa9jkL"
   },
   "source": [
    "### <span style=\"color:#0b486b\">4.4. Investigating the embedding vectors from the embedding matrix</span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 5 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT0Xea6C9jkL"
   },
   "source": [
    "**As you know, the embedding matrix is a collection of embedding vectors, each is for one word. In this part, you will base on the cosine similarity of the embedding vectors for the words to find the top-k most relevant words for a given word.**\n",
    "\n",
    "**Good embeddings should have words close in meaning near each other by some similarity metrics. The similarity metric we'll use is the `consine` distance, which is defined for two vector $\\mathbf{u}$ and $\\mathbf{v}$ as $\\cos(\\mathbf{u}, \\mathbf{v})=\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\left\\Vert{\\mathbf{u}}\\right\\Vert\\left\\Vert{\\mathbf{v}}\\right\\Vert}$ where $\\cdot$ means dot product and $\\left\\Vert\\cdot\\right\\Vert$ means the $\\mathcal{L}_2$ norm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsZEF3fb9jkL"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u,v):\n",
    "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pPE6A4P9jkL"
   },
   "source": [
    "#### <span style=\"color:red\">**Question 4.4.1**</span> \n",
    "\n",
    "**You are required to write the code for the function *find_most_similar(word= None, k=5, model= None)*. As its name, this function returns the top-k most relevant word for a given word based on the cosine similarity of the embedding vectors.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hmZWYvG9jkL"
   },
   "outputs": [],
   "source": [
    "def find_most_similar(word= None, k=5, model= None):\n",
    "    try:\n",
    "        table = model.word2vect.similar_by_word(word,k)#Insert your code here\n",
    "    except: \n",
    "        print(\"Word is not in the dictionary!\")\n",
    "    return table# a table of the top-k most relevant word for a given word based on the cosine similarity of the embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRSv1BrC9jkM"
   },
   "source": [
    "Here is the example of the above function. As you can observe, the result makes sense which demonstrates that we obtain a good model with the meaningful embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-QgOwpB9jkM",
    "outputId": "7a64f23e-7b21-44ba-8840-db50ce924977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hungary 0.7997257709503174\n",
      "lithuania 0.7970392107963562\n",
      "slovakia 0.7912642955780029\n",
      "bulgaria 0.7554062008857727\n",
      "germany 0.7552423477172852\n",
      "romania 0.7494610548019409\n",
      "warsaw 0.7206852436065674\n",
      "latvia 0.7173702716827393\n",
      "czech 0.7154167294502258\n",
      "polish 0.6944576501846313\n"
     ]
    }
   ],
   "source": [
    "table = find_most_similar(word='poland', k=10, model= my_best_rnn)\n",
    "for key,value in table:\n",
    "        print(key,value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0OIsknc9jkM"
   },
   "source": [
    "--- \n",
    "<div style=\"text-align: center\"> <span style=\"color:black\">END OF ASSIGNMENT</span> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T828l5t5TvY3",
    "outputId": "47abec6d-c822-40b7-a259-4e636b5a1e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | pattern '*.ipynb' matched no files\n",
      "This application is used to convert notebook files (*.ipynb)\n",
      "        to various other formats.\n",
      "\n",
      "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
      "\n",
      "Options\n",
      "=======\n",
      "The options below are convenience aliases to configurable class-options,\n",
      "as listed in the \"Equivalent to\" description-line of the aliases.\n",
      "To see all configurable class-options for some <cmd>, use:\n",
      "    <cmd> --help-all\n",
      "\n",
      "--debug\n",
      "    set log level to logging.DEBUG (maximize logging output)\n",
      "    Equivalent to: [--Application.log_level=10]\n",
      "--show-config\n",
      "    Show the application's configuration (human-readable format)\n",
      "    Equivalent to: [--Application.show_config=True]\n",
      "--show-config-json\n",
      "    Show the application's configuration (json format)\n",
      "    Equivalent to: [--Application.show_config_json=True]\n",
      "--generate-config\n",
      "    generate default config file\n",
      "    Equivalent to: [--JupyterApp.generate_config=True]\n",
      "-y\n",
      "    Answer yes to any questions instead of prompting.\n",
      "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
      "--execute\n",
      "    Execute the notebook prior to export.\n",
      "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
      "--allow-errors\n",
      "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
      "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
      "--stdin\n",
      "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
      "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
      "--stdout\n",
      "    Write notebook output to stdout instead of files.\n",
      "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
      "--inplace\n",
      "    Run nbconvert in place, overwriting the existing notebook (only \n",
      "            relevant when converting to notebook format)\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
      "--clear-output\n",
      "    Clear output of current file and save in place, \n",
      "            overwriting the existing notebook.\n",
      "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
      "--no-prompt\n",
      "    Exclude input and output prompts from converted document.\n",
      "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
      "--no-input\n",
      "    Exclude input cells and output prompts from converted document. \n",
      "            This mode is ideal for generating code-free reports.\n",
      "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True]\n",
      "--log-level=<Enum>\n",
      "    Set the log level by value or name.\n",
      "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
      "    Default: 30\n",
      "    Equivalent to: [--Application.log_level]\n",
      "--config=<Unicode>\n",
      "    Full path of a config file.\n",
      "    Default: ''\n",
      "    Equivalent to: [--JupyterApp.config_file]\n",
      "--to=<Unicode>\n",
      "    The export format to be used, either one of the built-in formats\n",
      "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides']\n",
      "            or a dotted object name that represents the import path for an\n",
      "            `Exporter` class\n",
      "    Default: 'html'\n",
      "    Equivalent to: [--NbConvertApp.export_format]\n",
      "--template=<Unicode>\n",
      "    Name of the template file to use\n",
      "    Default: ''\n",
      "    Equivalent to: [--TemplateExporter.template_file]\n",
      "--writer=<DottedObjectName>\n",
      "    Writer class used to write the \n",
      "                                        results of the conversion\n",
      "    Default: 'FilesWriter'\n",
      "    Equivalent to: [--NbConvertApp.writer_class]\n",
      "--post=<DottedOrNone>\n",
      "    PostProcessor class used to write the\n",
      "                                        results of the conversion\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
      "--output=<Unicode>\n",
      "    overwrite base name use for output files.\n",
      "                can only be used when converting one notebook at a time.\n",
      "    Default: ''\n",
      "    Equivalent to: [--NbConvertApp.output_base]\n",
      "--output-dir=<Unicode>\n",
      "    Directory to write output(s) to. Defaults\n",
      "                                  to output to the directory of each notebook. To recover\n",
      "                                  previous default behaviour (outputting to the current \n",
      "                                  working directory) use . as the flag value.\n",
      "    Default: ''\n",
      "    Equivalent to: [--FilesWriter.build_directory]\n",
      "--reveal-prefix=<Unicode>\n",
      "    The URL prefix for reveal.js (version 3.x).\n",
      "            This defaults to the reveal CDN, but can be any url pointing to a copy \n",
      "            of reveal.js. \n",
      "            For speaker notes to work, this must be a relative path to a local \n",
      "            copy of reveal.js: e.g., \"reveal.js\".\n",
      "            If a relative path is given, it must be a subdirectory of the\n",
      "            current directory (from which the server is run).\n",
      "            See the usage documentation\n",
      "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
      "            for more details.\n",
      "    Default: ''\n",
      "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
      "--nbformat=<Enum>\n",
      "    The nbformat version to write.\n",
      "            Use this to downgrade notebooks.\n",
      "    Choices: any of [1, 2, 3, 4]\n",
      "    Default: 4\n",
      "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      "    The simplest way to use nbconvert is\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb\n",
      "\n",
      "            which will convert mynotebook.ipynb to the default format (probably HTML).\n",
      "\n",
      "            You can specify the export format with `--to`.\n",
      "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
      "\n",
      "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
      "\n",
      "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
      "            'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
      "            can specify the flavor of the format used.\n",
      "\n",
      "            > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
      "\n",
      "            You can also pipe the output to stdout, rather than a file\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
      "\n",
      "            PDF is generated via latex\n",
      "\n",
      "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
      "\n",
      "            You can get (and serve) a Reveal.js-powered slideshow\n",
      "\n",
      "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
      "\n",
      "            Multiple notebooks can be given at the command line in a couple of \n",
      "            different ways:\n",
      "\n",
      "            > jupyter nbconvert notebook*.ipynb\n",
      "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
      "\n",
      "            or you can specify the notebooks list in a config file, containing::\n",
      "\n",
      "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
      "\n",
      "            > jupyter nbconvert --config mycfg.py\n",
      "\n",
      "To see all available configurables, use `--help-all`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html *.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CxozP0E99jkA",
    "opVK-UuI9jkB",
    "spmqJMQI9jkB",
    "E50FkAom9jkJ"
   ],
   "include_colab_link": true,
   "name": "Assignment02_solution.ipynp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
